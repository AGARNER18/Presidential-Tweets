# load packages
install.packages("tm")
install.packages("SnowballC")
install.packages("wordcloud")
install.packages("fpc")
install.packages("cluster")
install.packages("ggplot2")
install.packages("twitterR")
install.packages("RSentiment")
install.packages("devtools")
install.packages("DT")
install.packages("stm")
install.packages("topicmodels")
install.packages("wordcloud2")
install_url("http://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.2.tar.gz")
install.packages("plyr")
source("http://bioconductor.org/biocLite.R")
biocLite("Rgraphviz")
library(DT)
library(stm)
library(topicmodels)
library(wordcloud2)
library('RSentiment')
library("Rgraphviz")
library(sentiment)
library(devtools)
library(ggplot2)
library(cluster)
library(fpc)
library(wordcloud)
library(SnowballC)
library(tm)
library(twitteR)
library(plyr)


#********************************************Obama Tweets*************************************************************

# load tweets pulled from twitter made by Obama during his 2nd term as president 
# available in repository
obama <- read.csv("obama_tweets.csv")

# get only the text
some_txt = obama$text

# look at text before cleaning
some_txt[10]

# remove retweet
some_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", some_txt)

# remove at people
some_txt = gsub("@\\w+", "", some_txt)

# remove punctuation
some_txt = gsub("[[:punct:]]", "", some_txt)

# remove numbers
some_txt = gsub("[[:digit:]]", "", some_txt)

# remove html links
some_txt = gsub("http\\w+", "", some_txt)

# remove unnecessary spaces
some_txt = gsub("[ \t]{2,}", "", some_txt)
some_txt = gsub("^\\s+|\\s+$", "", some_txt)

# define "tolower error handling" function 
try.error = function(x)
{
  # create missing value
  y = NA
  # tryCatch error
  try_error = tryCatch(tolower(x), error=function(e) e)
  # if not an error
  if (!inherits(try_error, "error"))
    y = tolower(x)
  # result
  return(y)
}
# lower case using try.error with sapply 
some_txt = sapply(some_txt, try.error)

# remove NAs in some_txt
some_txt = some_txt[!is.na(some_txt)]
names(some_txt) = NULL

# look at text after cleaning
some_txt[3]

# emotion analysis on some_txt
emotion<-classify_emotion(some_txt, algorithm = "bayes", prior=1.0, verbose = TRUE)

# get only the emotion prediction
emotion_2 = emotion[,7]

# convert NA emotions to unkown
emotion_2[is.na(emotion_2)] = "unknown"

# polarity analysis on some_text
class_polarity= classify_polarity(some_txt, algorithm="bayes", verbose = TRUE)

# get only the polarity prediction
polarity = class_polarity[,4]

# create new data frame with the text, emotion prediction, and polarity prediction
sent_df = data.frame(text=some_txt, emotion=emotion_2, polarity=polarity, stringsAsFactors=FALSE)

# sort new data frame
sent_df = within(sent_df, emotion <- factor(emotion, levels=names(sort(table(emotion), decreasing=TRUE))))

# look at emotion and polarity classification of text previously viewed
sent_df[11,]

# plot distribution of emotions with unknown
ggplot(sent_df, aes(x=emotion)) +
  geom_bar(aes(y=..count.., fill=emotion)) +
  ggtitle("Emotions of Obama Tweets") +
  theme(plot.title = element_text(lineheight=.8, face="bold")) +
  scale_fill_brewer(palette="RdBu") +
  labs(x="emotion categories", y="number of tweets")

# subset to exclude rows where the emotion is unkown
no_un<-sent_df[sent_df$emotion != "unknown",]

# plot distribution of emotions without unknown
ggplot(no_un, aes(x=emotion)) +
  geom_bar(aes(y=..count.., fill=emotion)) +
  ggtitle("Emotions of Obama Tweets") +
  theme(plot.title = element_text(lineheight=.8, face="bold")) +
  scale_fill_brewer(palette="RdBu") +
  labs(x="emotion categories", y="number of tweets")
e
# plot distribution of polarity 
ggplot(sent_df, aes(x=polarity)) +
  geom_bar(aes(y=..count.., fill=polarity)) +
  ggtitle("Polarity of Obama Tweets") +
  theme(plot.title = element_text(lineheight=.8, face="bold")) +
  scale_fill_brewer(palette="RdBu") +
  labs(x="polarity categories", y="number of tweets")

# convert to corpus for word clouds
eall<-Corpus(VectorSource(sent_df$text))

# Look at conversion from df to corpus 
inspect(all[[3]])
sent_df[3,1]

# remove stop words
all<-tm_map(all, removeWords, stopwords("english"))
inspect(all[[5]])

e# remove word endings 
all<-tm_map(all, stemDocument)
inspect(all[[3]])

# remove url from document text
removeURL<-function(x)gsub("http[^[:space:]]*","",x)
all<-tm_map(all, content_transformer(removeURL))

# Get word frequencies
dtm<-DocumentTermMatrix(all)
kdtm

# calculate sum for how many times each term was used
freq<-colSums(as.matrix(dtm))
freq

# find number of distinct terms
length(freq)

#how many times each term appears in each document
m<-as.matrix(dtm)
m

# Create document term matrix 
dtm_up = DocumentTermMatrix(VCorpus(VectorSource(all)))
freq_up <- colSums(as.matrix(dtm_up))

# get sentiment for each term
sentiments_up = calculate_sentiment(names(freq_up))
sentiments_up = cbind(sentiments_up, as.data.frame(freq_up))

# create new df with only positive terms
sent_pos_up = sentiments_up[sentiments_up$sentiment == 'Positive',]

# create new df with only negative terms
sent_neg_up = sentiments_up[sentiments_up$sentiment == 'Negative',]

# compare number of positive terms to negative terms
cat("We have far lower negative Sentiments: ",sum(sent_neg_up$freq_up)," than positive: ",sum(sent_pos_up$freq_up))

DT::datatable(sent_pos_up)

# keep only the positive terms
positive<-sent_pos_up[,-2]

# keep only positive terms with frequency greater than 2
positive<-positive[positive$freq_up>2,]

# create word cloud of the positive terms
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
set.seed(100)
patriotic<-c("red","blue", "red3", "red4", "royalblue", "royalblue4")
wordcloud2(positive, size=1.3,gridSize=-3,minRotation = -pi/6, maxRotation = -pi/6, rotateRatio = 1,color=rep_len("red","blue", "red3", "red4", "royalblue", "royalblue4") ,nrow(positive))

# create word cloud of negative terms
DT::datatable(sent_neg_up)
negative <-sent_neg_up[,-2]layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
set.seed(100)
wordcloud2(negative, , shape="triangle",size=1.3,gridSize=-3,minRotation = -pi/-6, maxRotation = -pi/-6, rotateRatio = 1,color=rep_len( c("red","blue", "red3", "red4", "royalblue", "royalblue4"),nrow(negative)))

# get dimensions of number of documents and the number of distinct terms
dim(m)

# find frequent terms
findFreqTerms(dtm, lowfreq = 2)

# remove sparse terms that appear in less than 75% of documents
dtms<-removeSparseTerms(dtm, 0.75)
dtms

# find associations between words found in the same document
findAssocs(dtm, c("gun", "terrorist", "war"), corlimit=0.50)


#correlation plot

# build with 20 randomly selected words that appear at least 2 times with a correlation of 0.2 or greater
correlation_plot<-plot(dtm, terms=findFreqTerms(dtm, lowfreq = 2)[1:20], corThreshold = 0.2, weighting=T)

# word frequency plot that appears at least 10 times
wf<-data.frame(word=names(freq), freq=freq)
p<-ggplot(subset(wf, freq>10), aes(word, freq))
p<-p+ geom_bar(stat="identity", colour="black", fill="royalblue")
p<-p+ theme(axis.text.x = element_text(angle=45, hjust = 1))
all_freq_plot<-p
all_freq_plot

freq<-colSums(as.matrix(dtm))
# create word cloud of most frequently used words
wordcloud(names(freq), freq, min.freq =3000, max.words=150, color=c("royalblue3","red", "royalblue", "red3", "firebrick2"))

# k-means clustering
# cluster based on their appearance together
# remove the terms that appear less than 95% of documents
dtms<-removeSparseTerms(dtm, 0.95)
# build dissimiliatry matrix and store in d
d<-dist(t(dtms), method = "euclidian")
# use kmeans to build 4 cluster
kfit<-kmeans(d, 4)
# view clusters
kfit
# plot the clusters
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
# Find cluster details
kfit$cluster

# plot sum of squares distances between cluster and sum of squared distance between clusters for k between 2 and 15
# find optimal level of clusters for kmeans
bss<-integer(length(2:15))
for (i in 2:15) bss[i] <- kmeans(d,centers=i)$betweenss
plot(1:15, bss, type="b", xlab="Number of Clusters",  ylab="Sum of squares", col="blue") 
wss<-integer(length(2:15))
for (i in 2:15) wss[i] <- kmeans(d,centers=i)$tot.withinss
lines(1:15, wss, type="b" )

# build dissimiilarity matrix 
d<-dist(t(dtms), method = "euclidian")
# build dendogram and store in variable fit
fit<-hclust(d=d, method="ward.D2")
# plot dendogram
plot(fit, hang=1)
# add red line to show the cluster each belows to 
plot.new()
plot(fit, hang = -1)
groups<-cutree(fit, k=4)
rect.hclust(fit, k=4, border = "red")

# check if there are enough terms for topic analysis
rowsum.dtm<-apply(dtm, 1, sum)
dtms[rowsum.dtm>0,]
# create LDA with 9 topics
lda.obama.notes<-LDA(dtm[rowsum.dtm>0,], k=9, method="Gibbs", control=list(nstart=5, seed=list(101, 102, 103, 104, 105), best=TRUE, burnin=4000, iter=2000, thin=500))
# view details of LDA
summary(lda.obama.notes)
# get 9 topics
topics.terms<-terms(lda.obama.notes,9)
# view topics
knitr::kable(topics.terms)
# view all words in each topic
topics.terms.lst<-apply(topics.terms, MARGIN = 2, paste, collapse=", ")
write(topics.terms.lst,"", 40, sep = "\n")

# create barplot to show how often each topic appears in the documents
topics.obama.notes<-topics(lda.obama.notes, 1)
topics.term.freq<-rowSums(as.matrix(dtm[rowsum.dtm>0,]))
topic.df<-data.frame(topics.term.freq, topics.obama.notes)
knitr::kable(head(topic.df))
barplot(table(topic.df$topics.obama.notes), names.arg = c("Topic-1", "Topic-2", "Topic-3", "Topic-4", "Topic-5", "Topic-6", "Topic-7", "Topic-8", "Topic-9"), col = blues9, main = "Total Documents Per Topic", ylab = "Documents")

# create density plot of topics
ob<-qplot(topics.term.freq,..count.., data = topic.df, geom = "density", fill=topics.terms.lst[topics.obama.notes], xlab = "Total Terms in Topics", ylab = "Documents Covered By Topics") 
ob<- ob + scale_fill_brewer(palette="RdBu")
ob<-ob+ labs(fill="Topics")
ob<-ob + geom_density(alpha=.4)
ob
